{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Image Classifier layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "from two_layer_perceptron import TwoLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipynb parameters\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline \n",
    "# in the notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = TwoLayerPerceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking Affine Forward and Backward layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs of shape:  (10, 6)\n",
      "[[ 0.41794341  1.39710028 -1.78590431 -0.70882773 -0.07472532 -0.77501677]\n",
      " [-0.1497979   1.86172902 -1.4255293  -0.3763567  -0.34227539  0.29490764]\n",
      " [-0.83732373  0.95218767  1.32931659  0.52465245 -0.14809998  0.88953195]\n",
      " [ 0.12444653  0.99109251  0.03514666  0.26207083  0.14320173  0.90101716]\n",
      " [ 0.23185863 -0.79725793  0.12001014 -0.65679608  0.26917456  0.333667  ]\n",
      " [ 0.27423503  0.76215717 -0.69550058  0.29214712 -0.38489942  0.1228747 ]\n",
      " [-1.42904497  0.70286283 -0.85850947 -1.14042979 -1.58535997 -0.01530138]\n",
      " [-0.32156083  0.56834936 -0.19961722  1.27286625  1.27292534  1.58102968]\n",
      " [-1.75626715  0.9217743  -0.6753054  -1.43443616  0.47021125  0.03196734]\n",
      " [ 0.04448574  0.47824879 -2.51335181 -1.15740245 -0.70470413 -1.04978879]]\n",
      "Weights of shape:  (6, 5)\n",
      "[[-1.90795589  0.49258765  0.83736166 -1.4288134  -0.18982427]\n",
      " [-1.14094943 -2.12570755 -0.41354791  0.44148975  0.16411113]\n",
      " [-0.65505065 -0.30212765 -0.25704466 -0.12841368  0.26338593]\n",
      " [ 0.1672181  -0.30871951 -1.26754462 -0.22319022 -0.82993433]\n",
      " [-1.11271826 -0.44613095 -0.40001719  0.36343905  0.94992777]\n",
      " [-0.32379447  0.27031704 -0.63381148 -2.71484268  0.65576139]]\n",
      "Bias of shape:  (5,)\n",
      "[-1.17004858  0.0598685  -1.64182729 -0.28069634 -0.67946972]\n"
     ]
    }
   ],
   "source": [
    "# generating random data to test affine layer implementation\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "print(\"Inputs of shape: \", x.reshape((x.shape[0], -1)).shape)\n",
    "print(x.reshape((x.shape[0], -1)))\n",
    "w = np.random.randn(6, 5)\n",
    "print(\"Weights of shape: \", w.shape)\n",
    "print(w)\n",
    "b = np.random.randn(5)\n",
    "print(\"Bias of shape: \", b.shape)\n",
    "print(b)\n",
    "dout = np.random.randn(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Affine Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of shape :  (10, 5)\n",
      "[[-2.17606361 -2.12184729  0.00900847  2.20337442 -0.99083714]\n",
      " [-1.85215259 -3.19211285 -1.74370267  0.0973037  -0.54036486]\n",
      " [-1.56514011 -2.63372545 -4.24801244 -1.42050022 -0.00692815]\n",
      " [-2.96856091 -1.89745509 -2.91706368 -2.47802971 -0.02180427]\n",
      " [-1.29878952  2.00543255 -0.63545946 -1.64080561  0.1968879 ]\n",
      " [-1.66992084 -1.10030085 -1.84282972 -0.78540801 -1.31714587]\n",
      " [ 2.89525541 -0.82354792 -0.81903185  1.90158524 -1.08850368]\n",
      " [-2.78971285 -1.77983379 -5.21950069 -3.6583993   0.61183637]\n",
      " [ 0.79806586 -2.31893721 -1.71021427  3.12661299  1.28543738]\n",
      " [ 0.77630397  0.21244488  1.25800943  3.04184783 -1.66867068]]\n"
     ]
    }
   ],
   "source": [
    "scores, (x, w, b) = perceptron.affine_forward(x, w, b)\n",
    "print(\"Scores of shape : \", scores.shape)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Affine Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input gradient of shape:  (10, 6)\n",
      "[[ 5.06973048  0.52895344  1.22136172 -0.19561005  1.56951029  3.72781223]\n",
      " [-1.01960674  3.16856234  0.66629986  1.12662151  1.15800203 -0.33676616]\n",
      " [ 0.01917811  0.49589462  0.1643096   0.57856225  0.34138118 -1.20252682]\n",
      " [-1.00781287  2.39378843  0.12283772  0.90902209  0.19081771 -2.97852996]\n",
      " [ 2.36599088  1.79213685  1.11158625 -1.23937492  2.25219041  2.07000103]\n",
      " [ 0.95915612  2.69695394  0.88221926 -0.3273759   1.84259405 -0.60336282]\n",
      " [ 6.47924185  1.58133859  1.16457556 -2.19076747  1.03404647  6.20462543]\n",
      " [ 2.69588666  1.1217322   1.2400047   4.14104512  0.52346801  5.6311282 ]\n",
      " [-4.16910147 -3.93402396 -1.57652706  1.72466666 -2.93171083 -0.69903127]\n",
      " [ 1.17992956  1.11091973  0.95563638  1.15028299  1.40801655  3.06403357]]\n",
      "Weights gradient of shape:  (6, 5)\n",
      "[[-2.37342917 -0.51198268  0.31810037  2.90403428  1.06935402]\n",
      " [-3.51268592 -2.11141032 -3.63676781 -1.97058092 -2.23413933]\n",
      " [ 5.08663365  1.49488732  3.34009108  5.93662486 -2.15844283]\n",
      " [-0.69809993 -0.20996862 -2.34841896  2.76050051 -3.16921717]\n",
      " [ 2.22793491  2.34320739 -4.92577398  2.06883897 -3.34916043]\n",
      " [-0.77798671 -1.29867108 -3.14496814  0.15478615 -3.09581511]]\n",
      "Biases gradient of shape:  (5,)\n",
      "[-5.78588657 -2.14288946 -3.93648137 -4.10664587 -0.09253319]\n"
     ]
    }
   ],
   "source": [
    "_, (x, w, b) = perceptron.affine_forward(x, w, b)\n",
    "dx, dw, db = perceptron.affine_backward(dout, cache=(x, w, b))\n",
    "\n",
    "print(\"Input gradient of shape: \", dx.shape)\n",
    "print(dx)\n",
    "print(\"Weights gradient of shape: \", dw.shape)\n",
    "print(dw)\n",
    "print(\"Biases gradient of shape: \", db.shape)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating analytical gradients with numercial to check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute numerical gradient\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# function to compute relative error\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.0908199508708189e-10\n",
      "dw error:  2.1752635504596857e-10\n",
      "db error:  7.736978834487815e-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: perceptron.affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: perceptron.affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: perceptron.affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = perceptron.affine_forward(x, w, b)\n",
    "dx, dw, db = perceptron.affine_backward(dout, cache)\n",
    "\n",
    "dx_num = dx_num.reshape((dx_num.shape[0], -1))\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking ReLu activation function\n",
    "1. ReLu Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores of shape (10, 5)\n",
      "[[ 0.41794341  1.39710028 -1.78590431 -0.70882773 -0.07472532]\n",
      " [-0.77501677 -0.1497979   1.86172902 -1.4255293  -0.3763567 ]\n",
      " [-0.34227539  0.29490764 -0.83732373  0.95218767  1.32931659]\n",
      " [ 0.52465245 -0.14809998  0.88953195  0.12444653  0.99109251]\n",
      " [ 0.03514666  0.26207083  0.14320173  0.90101716  0.23185863]\n",
      " [-0.79725793  0.12001014 -0.65679608  0.26917456  0.333667  ]\n",
      " [ 0.27423503  0.76215717 -0.69550058  0.29214712 -0.38489942]\n",
      " [ 0.1228747  -1.42904497  0.70286283 -0.85850947 -1.14042979]\n",
      " [-1.58535997 -0.01530138 -0.32156083  0.56834936 -0.19961722]\n",
      " [ 1.27286625  1.27292534  1.58102968 -1.75626715  0.9217743 ]]\n",
      "Scores after activation : (10, 5)\n",
      "[[0.41794341 1.39710028 0.         0.         0.        ]\n",
      " [0.         0.         1.86172902 0.         0.        ]\n",
      " [0.         0.29490764 0.         0.95218767 1.32931659]\n",
      " [0.52465245 0.         0.88953195 0.12444653 0.99109251]\n",
      " [0.03514666 0.26207083 0.14320173 0.90101716 0.23185863]\n",
      " [0.         0.12001014 0.         0.26917456 0.333667  ]\n",
      " [0.27423503 0.76215717 0.         0.29214712 0.        ]\n",
      " [0.1228747  0.         0.70286283 0.         0.        ]\n",
      " [0.         0.         0.         0.56834936 0.        ]\n",
      " [1.27286625 1.27292534 1.58102968 0.         0.9217743 ]]\n",
      "Testing relu_backward function:\n",
      "dx error:  3.2756219598992723e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 5)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: perceptron.relu_forward(x)[0], x, dout)\n",
    "print(\"scores of shape\", x.shape)\n",
    "print(x)\n",
    "scores_after_activation , x = perceptron.relu_forward(x)\n",
    "print(\"Scores after activation :\", scores_after_activation.shape)\n",
    "print(scores_after_activation)\n",
    "dx = perceptron.relu_backward(dout, x)\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU\n",
    "\n",
    "## Answer:\n",
    "As a result of backpropagation, gradient flow near zero leads to the vanishing gradient problem.\n",
    "\n",
    "1. The sigmoid function suffers from the vanishing gradient problem because the gradient is close to zero for very large positive and negative input values. \n",
    "\n",
    "2. Due to its linear response to a positive input, ReLU has the advantage over Sigmoid of being less susceptible to the vanishing gradient problem. For negative inputs, ReLU's gradient is 0; for positive inputs, it is 1. It is not very likely that ReLU would suffer from the vanishing gradient problem if all input values are negative. This results in some neurons not being able to train further. There is a problem called \"dying ReLU\". \n",
    "\n",
    "3. Leaky ReLU attempts to solve the \"dead\" neuron ReLU problem by applying a small negative gradient to negative values. That is, 0.01*X if x < 0, and x otherwise. Leaky ReLU therefore aims to solve the vanishing gradient problem. However, the function max(0.01x, x) is not continuous at x = 0, so the slope at x = 0 is undefined. So, if not explicitly handled in your code, a one-dimensional example where the gradient can be zero is to consider all zero values. [0, 0, 0] only occurs if the network initialization is incorrect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss layers: Softmax and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to eva;uate numerical gradient\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f at x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    fx = f(x)  # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext()  # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.999602749096233\n",
      "dx error:  1.4021566006651672e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302545844500738\n",
      "dx error:  9.483503037636722e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: perceptron.svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = perceptron.svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: perceptron.softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = perceptron.softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
